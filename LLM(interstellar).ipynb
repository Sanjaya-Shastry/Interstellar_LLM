{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3870b2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86957a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/sanjayashastry/Downloads/interstellar_script.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb8c954f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.lower()\n",
    "lines = text.splitlines()\n",
    "lines = [line.strip() for line in lines if line.strip()]\n",
    "text = ' '.join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "92952d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed = set(string.ascii_letters + string.digits + ' ')\n",
    "text = ''.join(c for c in text if c in allowed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fba53e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "tokens = text.split()\n",
    "vocab = sorted(set(tokens))\n",
    "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "idx_to_word = {i: word for word, i in word_to_idx.items()}\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb252801",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = np.array([word_to_idx[word] for word in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7f218fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "context_size = 6\n",
    "X, y = [], []\n",
    "for i in range(context_size, len(token_ids)):\n",
    "    X.append(token_ids[i - context_size:i])\n",
    "    y.append(token_ids[i])\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c4648e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional encoding\n",
    "def get_positional_encoding(x, y):\n",
    "    encoding = np.zeros((x, y))\n",
    "    for i in range(x):\n",
    "        for j in range(y):\n",
    "            angle = i / np.power(10000, (2 * (j // 2)) / y)\n",
    "            encoding[i, j] = np.sin(angle) if j % 2 == 0 else np.cos(angle)\n",
    "    return tf.convert_to_tensor(encoding, dtype=tf.float32)\n",
    "\n",
    "#basically, we would want x= context sixe and y= embed dim, hence the output is a 6 by 64 matrix with each row representing the pos enc for eacg word in each element of X (uff thats a lot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0fd31c3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6, 64), dtype=float32, numpy=\n",
       "array([[ 0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00],\n",
       "       [ 8.41470957e-01,  5.40302277e-01,  6.81561351e-01,\n",
       "         7.31760979e-01,  5.33168435e-01,  8.46009135e-01,\n",
       "         4.09308910e-01,  9.12395835e-01,  3.10983598e-01,\n",
       "         9.50415254e-01,  2.34921083e-01,  9.72014427e-01,\n",
       "         1.76892191e-01,  9.84230220e-01,  1.32957265e-01,\n",
       "         9.91121769e-01,  9.98334140e-02,  9.95004177e-01,\n",
       "         7.49191567e-02,  9.97189581e-01,  5.62044978e-02,\n",
       "         9.98419285e-01,  4.21571545e-02,  9.99110997e-01,\n",
       "         3.16175073e-02,  9.99500036e-01,  2.37115137e-02,\n",
       "         9.99718845e-01,  1.77818574e-02,  9.99841869e-01,\n",
       "         1.33348191e-02,  9.99911070e-01,  9.99983307e-03,\n",
       "         9.99949992e-01,  7.49887200e-03,  9.99971867e-01,\n",
       "         5.62338345e-03,  9.99984205e-01,  4.21695272e-03,\n",
       "         9.99991119e-01,  3.16227227e-03,  9.99994993e-01,\n",
       "         2.37137149e-03,  9.99997199e-01,  1.77827850e-03,\n",
       "         9.99998391e-01,  1.33352098e-03,  9.99999106e-01,\n",
       "         9.99999815e-04,  9.99999523e-01,  7.49894127e-04,\n",
       "         9.99999702e-01,  5.62341302e-04,  9.99999821e-01,\n",
       "         4.21696488e-04,  9.99999940e-01,  3.16227757e-04,\n",
       "         9.99999940e-01,  2.37137370e-04,  1.00000000e+00,\n",
       "         1.77827940e-04,  1.00000000e+00,  1.33352136e-04,\n",
       "         1.00000000e+00],\n",
       "       [ 9.09297407e-01, -4.16146845e-01,  9.97479975e-01,\n",
       "         7.09482506e-02,  9.02130723e-01,  4.31462824e-01,\n",
       "         7.46903539e-01,  6.64932430e-01,  5.91127098e-01,\n",
       "         8.06578398e-01,  4.56693351e-01,  8.89624178e-01,\n",
       "         3.48205268e-01,  9.37418282e-01,  2.63553679e-01,\n",
       "         9.64644730e-01,  1.98669329e-01,  9.80066597e-01,\n",
       "         1.49417207e-01,  9.88774240e-01,  1.12231314e-01,\n",
       "         9.93682086e-01,  8.42393488e-02,  9.96445537e-01,\n",
       "         6.32033944e-02,  9.98000681e-01,  4.74096946e-02,\n",
       "         9.98875499e-01,  3.55580896e-02,  9.99367595e-01,\n",
       "         2.66672671e-02,  9.99644339e-01,  1.99986659e-02,\n",
       "         9.99800026e-01,  1.49973221e-02,  9.99887526e-01,\n",
       "         1.12465890e-02,  9.99936759e-01,  8.43382999e-03,\n",
       "         9.99964416e-01,  6.32451335e-03,  9.99979973e-01,\n",
       "         4.74272948e-03,  9.99988735e-01,  3.55655141e-03,\n",
       "         9.99993682e-01,  2.66703963e-03,  9.99996424e-01,\n",
       "         1.99999870e-03,  9.99997973e-01,  1.49978790e-03,\n",
       "         9.99998868e-01,  1.12468237e-03,  9.99999344e-01,\n",
       "         8.43392918e-04,  9.99999642e-01,  6.32455514e-04,\n",
       "         9.99999821e-01,  4.74274711e-04,  9.99999881e-01,\n",
       "         3.55655880e-04,  9.99999940e-01,  2.66704272e-04,\n",
       "         9.99999940e-01],\n",
       "       [ 1.41120002e-01, -9.89992499e-01,  7.78272510e-01,\n",
       "        -6.27926648e-01,  9.93253171e-01, -1.15966141e-01,\n",
       "         9.53634441e-01,  3.00967306e-01,  8.12648892e-01,\n",
       "         5.82753599e-01,  6.52904034e-01,  7.57440686e-01,\n",
       "         5.08536160e-01,  8.61040652e-01,  3.89470309e-01,\n",
       "         9.21039045e-01,  2.95520216e-01,  9.55336511e-01,\n",
       "         2.23075420e-01,  9.74801183e-01,  1.67903304e-01,\n",
       "         9.85803485e-01,  1.26171768e-01,  9.92008388e-01,\n",
       "         9.47260931e-02,  9.95503366e-01,  7.10812211e-02,\n",
       "         9.97470558e-01,  5.33230789e-02,  9.98577297e-01,\n",
       "         3.99949737e-02,  9.99199867e-01,  2.99955010e-02,\n",
       "         9.99550045e-01,  2.24949289e-02,  9.99746978e-01,\n",
       "         1.68694388e-02,  9.99857724e-01,  1.26505578e-02,\n",
       "         9.99919951e-01,  9.48669109e-03,  9.99954998e-01,\n",
       "         7.11406115e-03,  9.99974668e-01,  5.33481315e-03,\n",
       "         9.99985754e-01,  4.00055340e-03,  9.99992013e-01,\n",
       "         2.99999560e-03,  9.99995530e-01,  2.24968069e-03,\n",
       "         9.99997497e-01,  1.68702321e-03,  9.99998569e-01,\n",
       "         1.26508914e-03,  9.99999225e-01,  9.48683184e-04,\n",
       "         9.99999523e-01,  7.11412053e-04,  9.99999762e-01,\n",
       "         5.33483806e-04,  9.99999881e-01,  4.00056422e-04,\n",
       "         9.99999940e-01],\n",
       "       [-7.56802499e-01, -6.53643608e-01,  1.41538918e-01,\n",
       "        -9.89932716e-01,  7.78471768e-01, -6.27679646e-01,\n",
       "         9.93280709e-01, -1.15729779e-01,  9.53580737e-01,\n",
       "         3.01137477e-01,  8.12570930e-01,  5.82862377e-01,\n",
       "         6.52827978e-01,  7.57506192e-01,  5.08471310e-01,\n",
       "         8.61078918e-01,  3.89418334e-01,  9.21060979e-01,\n",
       "         2.95479774e-01,  9.55348969e-01,  2.23044485e-01,\n",
       "         9.74808276e-01,  1.67879850e-01,  9.85807478e-01,\n",
       "         1.26154065e-01,  9.92010653e-01,  9.47127715e-02,\n",
       "         9.95504618e-01,  7.10712075e-02,  9.97471273e-01,\n",
       "         5.33155650e-02,  9.98577714e-01,  3.99893336e-02,\n",
       "         9.99200106e-01,  2.99912710e-02,  9.99550164e-01,\n",
       "         2.24917568e-02,  9.99747038e-01,  1.68670602e-02,\n",
       "         9.99857724e-01,  1.26487734e-02,  9.99920011e-01,\n",
       "         9.48535278e-03,  9.99954998e-01,  7.11305765e-03,\n",
       "         9.99974728e-01,  5.33406064e-03,  9.99985754e-01,\n",
       "         3.99998948e-03,  9.99992013e-01,  2.99957232e-03,\n",
       "         9.99995530e-01,  2.24936334e-03,  9.99997497e-01,\n",
       "         1.68678525e-03,  9.99998569e-01,  1.26491068e-03,\n",
       "         9.99999225e-01,  9.48549365e-04,  9.99999523e-01,\n",
       "         7.11311703e-04,  9.99999762e-01,  5.33408544e-04,\n",
       "         9.99999881e-01],\n",
       "       [-9.58924294e-01,  2.83662200e-01, -5.71127176e-01,\n",
       "        -8.20861578e-01,  3.23935211e-01, -9.46079254e-01,\n",
       "         8.58896017e-01, -5.12150049e-01,  9.99946535e-01,\n",
       "        -1.03423186e-02,  9.26757336e-01,  3.75660598e-01,\n",
       "         7.76529968e-01,  6.30080283e-01,  6.18443727e-01,\n",
       "         7.85829127e-01,  4.79425550e-01,  8.77582550e-01,\n",
       "         3.66223305e-01,  9.30526972e-01,  2.77480543e-01,\n",
       "         9.60731268e-01,  2.09289446e-01,  9.77853715e-01,\n",
       "         1.57455891e-01,  9.87526000e-01,  1.18291065e-01,\n",
       "         9.92978990e-01,  8.87968615e-02,  9.96049762e-01,\n",
       "         6.66266754e-02,  9.97777998e-01,  4.99791689e-02,\n",
       "         9.98750269e-01,  3.74859273e-02,  9.99297142e-01,\n",
       "         2.81133614e-02,  9.99604762e-01,  2.10832637e-02,\n",
       "         9.99777734e-01,  1.58107299e-02,  9.99875009e-01,\n",
       "         1.18565904e-02,  9.99929726e-01,  8.89127981e-03,\n",
       "         9.99960482e-01,  6.66755764e-03,  9.99977767e-01,\n",
       "         4.99997940e-03,  9.99987483e-01,  3.74946231e-03,\n",
       "         9.99992967e-01,  2.81170290e-03,  9.99996066e-01,\n",
       "         2.10848101e-03,  9.99997795e-01,  1.58113812e-03,\n",
       "         9.99998748e-01,  1.18568656e-03,  9.99999285e-01,\n",
       "         8.89139599e-04,  9.99999583e-01,  6.66760665e-04,\n",
       "         9.99999762e-01]], dtype=float32)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_positional_encoding(6,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "837e2c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class MiniLLM(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, context_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embed = layers.Embedding(vocab_size, embed_dim)  #we are embedding (into dim 64)\n",
    "        self.W_q = self.add_weight(shape=(embed_dim, embed_dim), initializer='random_normal', trainable=True)\n",
    "        self.W_k = self.add_weight(shape=(embed_dim, embed_dim), initializer='random_normal', trainable=True)\n",
    "        self.W_v = self.add_weight(shape=(embed_dim, embed_dim), initializer='random_normal', trainable=True)\n",
    "        self.out = layers.Dense(vocab_size)\n",
    "        self.pos_enc = get_positional_encoding(context_size, embed_dim)\n",
    "        self.context_size = context_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.out.build((None, self.context_size * self.embed_dim))\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.embed(x) + self.pos_enc  # (batch, context, dim)\n",
    "        Q = tf.matmul(x, self.W_q)\n",
    "        K = tf.matmul(x, self.W_k)\n",
    "        V = tf.matmul(x, self.W_v)\n",
    "        scale = tf.math.sqrt(tf.cast(self.embed_dim, tf.float32))\n",
    "        scores = tf.matmul(Q, K, transpose_b=True) / scale\n",
    "        weights = tf.nn.softmax(scores, axis=-1)\n",
    "        output = tf.matmul(weights, V)\n",
    "        flat = tf.reshape(output, (tf.shape(x)[0], -1))\n",
    "        return self.out(flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9f95cdd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.0474 - loss: 6.8021\n",
      "Epoch 2/20\n",
      "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.0669 - loss: 6.0769\n",
      "Epoch 3/20\n",
      "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.0987 - loss: 5.7438\n",
      "Epoch 4/20\n",
      "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - accuracy: 0.1318 - loss: 5.3671\n",
      "Epoch 5/20\n",
      "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.1525 - loss: 5.0687\n",
      "Epoch 6/20\n",
      "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.1786 - loss: 4.7781\n",
      "Epoch 7/20\n",
      "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.2019 - loss: 4.5466\n",
      "Epoch 8/20\n",
      "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.2172 - loss: 4.2674\n",
      "Epoch 9/20\n",
      "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - accuracy: 0.2301 - loss: 4.0919\n",
      "Epoch 10/20\n",
      "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - accuracy: 0.2493 - loss: 3.8807\n",
      "Epoch 11/20\n",
      "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - accuracy: 0.2651 - loss: 3.7299\n",
      "Epoch 12/20\n",
      "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - accuracy: 0.2725 - loss: 3.5741\n",
      "Epoch 13/20\n",
      "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - accuracy: 0.2940 - loss: 3.4223\n",
      "Epoch 14/20\n",
      "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 17ms/step - accuracy: 0.2952 - loss: 3.2965\n",
      "Epoch 15/20\n",
      "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - accuracy: 0.3108 - loss: 3.2042\n",
      "Epoch 16/20\n",
      "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - accuracy: 0.3237 - loss: 3.0900\n",
      "Epoch 17/20\n",
      "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - accuracy: 0.3243 - loss: 3.0547\n",
      "Epoch 18/20\n",
      "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.3385 - loss: 2.9551\n",
      "Epoch 19/20\n",
      "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.3445 - loss: 2.8929\n",
      "Epoch 20/20\n",
      "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.3526 - loss: 2.8536\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x13ea3b820>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "embed_dim = 64\n",
    "model = MiniLLM(vocab_size, context_size, embed_dim)\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2f9d1df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation\n",
    "def generate(model, prompt, num_words, temperature=0.7):\n",
    "    words = prompt.lower().split()\n",
    "    result = words[:]\n",
    "    for i in range(num_words):\n",
    "        context = [word_to_idx.get(w, 0) for w in result[-context_size:]]\n",
    "        if len(context) < context_size:\n",
    "            context = [0] * (context_size - len(context)) + context\n",
    "        input_tensor = np.array(context).reshape(1, -1)\n",
    "        logits = model(input_tensor)[0] / temperature\n",
    "        probs = tf.nn.softmax(logits).numpy()\n",
    "        next_id = np.random.choice(len(probs), p=probs)\n",
    "        result.append(idx_to_word[next_id])\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "10fbc690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love is about people to find on the ship hits a hot\n"
     ]
    }
   ],
   "source": [
    "print(generate(model, \"Love is \", num_words=10, temperature=0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f03f521",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
